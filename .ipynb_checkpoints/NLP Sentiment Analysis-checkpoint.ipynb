{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json as js\n",
    "import sys\n",
    "import pip\n",
    "import pprint \n",
    "import os\n",
    "# ! pip install --user plotly\n",
    "import plotly.figure_factory as ff\n",
    "import re\n",
    "import csv\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweets only from Illinois\n",
    "def get_tweets_from_illinois(tweets):\n",
    "\n",
    "    il_tweets = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            Tweet1 = js.loads(tweet)\n",
    "            if (Tweet1[\"place\"] != None):\n",
    "                if (Tweet1[\"place\"][\"name\"] == 'Chicago' or (\", IL\" in Tweet1[\"place\"][\"full_name\"])):\n",
    "                    il_tweets.append(Tweet1)\n",
    "        except KeyError as e:\n",
    "            continue\n",
    "        except ValueError as e:\n",
    "            continue\n",
    "        \n",
    "    return il_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-23-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-22-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-17-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-18-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-04-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-20-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-21-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-12-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-18-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-05-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-12-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-19-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-19-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-04-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-21-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-10-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-15-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-07-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-02-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-17-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-11-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-23-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-14-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-16-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-03-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-08-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-00-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-02-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-22-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-16-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-06-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-09-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-07-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-09-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-14-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-15-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-05-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-11-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-13-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-01-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-20-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-08-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-13-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-03-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-01-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-06-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-10-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-25/tweets.log.2020-06-25-00-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-01-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-22-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-14-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-16-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-03-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-15-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-23-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-20-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-07-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-16-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-02-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-01-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-23-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-19-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-02-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-14-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-04-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-12-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-10-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-00-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-09-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-05-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-03-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-13-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-17-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-11-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-06-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-21-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-04-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-18-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-13-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-15-00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-08-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-00-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-19-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-10-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-20-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-12-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-07-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-08-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-09-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-06-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-18-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-05-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-17-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-22-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-11-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-29/tweets.log.2020-06-29-21-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-03-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-23-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-16-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-01-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-02-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-19-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-13-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-22-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-00-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-01-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-14-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-12-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-04-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-09-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-17-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-22-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-10-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-08-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-13-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-12-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-07-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-06-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-02-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-23-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-18-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-07-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-18-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-11-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-09-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-11-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-06-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-05-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-20-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-20-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-16-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-21-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-15-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-10-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-17-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-19-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-00-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-21-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-03-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-08-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-04-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-15-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-05-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-20/tweets.log.2020-06-20-14-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-02-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-18-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-05-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-02-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-16-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-21-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-07-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-00-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-12-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-17-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-14-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-20-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-22-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-09-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-06-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-00-30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-06-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-11-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-15-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-01-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-19-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-09-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-15-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-11-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-07-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-13-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-16-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-08-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-21-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-04-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-04-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-18-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-12-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-17-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-08-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-14-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-20-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-10-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-05-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-19-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-23-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-13-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-01-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-10-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-23-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-03-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-22-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-26/tweets.log.2020-06-26-03-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-22-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-18-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-15-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-00-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-17-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-20-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-17-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-08-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-05-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-05-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-10-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-04-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-12-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-22-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-23-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-01-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-07-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-15-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-04-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-14-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-07-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-10-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-21-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-01-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-06-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-16-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-13-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-11-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-19-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-02-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-02-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-14-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-08-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-03-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-21-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-13-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-12-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-19-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-11-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-16-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-06-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-23-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-09-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-09-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-20-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-03-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-00-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-10/tweets.log.2020-06-10-18-30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-13-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-14-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-12-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-01-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-18-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-08-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-22-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-03-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-19-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-09-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-07-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-05-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-01-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-23-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-20-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-16-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-17-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-02-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-15-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-00-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-06-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-10-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-21-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-03-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-10-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-22-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-20-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-04-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-13-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-19-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-08-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-21-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-07-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-11-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-09-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-15-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-11-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-06-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-16-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-12-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-17-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-04-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-18-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-02-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-00-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-05-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-23-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-27/tweets.log.2020-06-27-14-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-01-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-13-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-05-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-17-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-16-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-00-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-07-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-16-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-00-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-06-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-20-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-17-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-10-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-23-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-15-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-05-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-21-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-11-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-18-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-22-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-10-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-09-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-20-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-22-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-03-30\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-12-00\n",
      "Gathering data from /scratch/00713/kelly/tweets_raw/tweets/june/2020-06-12/tweets.log.2020-06-12-08-00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e93e21f775b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gathering data from \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtweet_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tweets_from_illinois\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mtotal_tweets\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#             for t in tweet_array:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a6c107227b56>\u001b[0m in \u001b[0;36mget_tweets_from_illinois\u001b[0;34m(tweets)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mTweet1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTweet1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"place\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTweet1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"place\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Chicago'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\", IL\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTweet1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"place\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"full_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/intel18/python3/3.7.0/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/intel18/python3/3.7.0/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/intel18/python3/3.7.0/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path as pth\n",
    "\n",
    "il_text_file = open(\"/scratch/07470/sailia/all_june_tweets.txt\", \"w\")\n",
    "total_tweets = 0\n",
    "# csvFile = pd.read_csv('/home1/07470/sailia/twitter/tweets.log.2020-03-25-15-00.csv')\n",
    "\n",
    "for path, dirs, files in os.walk(\"/scratch/00713/kelly/tweets_raw/tweets/june\"):\n",
    "    for f in files:\n",
    "        fileName = os.path.join(path, f)\n",
    "        with open(fileName, \"r\") as myFile:\n",
    "            print(\"Gathering data from \" + fileName)\n",
    "            tweets = myFile.readlines()\n",
    "            tweet_array = get_tweets_from_illinois(tweets)\n",
    "            total_tweets += len(tweet_array)\n",
    "            for t in tweet_array:\n",
    "            il_text_file.write(js.dumps(tweets) + \"\\n\")\n",
    "\n",
    "il_text_file.close()\n",
    "# print(csvFile.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_text_file = open(\"/scratch/07470/sailia/_tweets.txt\", \"w\")\n",
    "\n",
    "# csvFile = pd.read_csv('/home1/07470/sailia/twitter/tweets.log.2020-03-25-15-00.csv')\n",
    "\n",
    "for path, dirs, files in os.walk(\"/scratch/00713/kelly/tweets_raw/tweets/june\"):\n",
    "    for f in files:\n",
    "        fileName = os.path.join(path, f)\n",
    "#         if(pth(fileName).name.startswith(\"2020-03\")):\n",
    "        with open(fileName, \"r\") as myFile:\n",
    "            print(\"Gathering data from \" + fileName)\n",
    "            tweets = myFile.readlines()\n",
    "            tweet_array = get_tweets_from_illinois(tweets)\n",
    "            for t in tweet_array:\n",
    "                il_text_file.write(js.dumps(tweets) + \"\\n\")\n",
    "\n",
    "il_text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type '_io.TextIOWrapper' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7d910bc04b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mil_tweets_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_tweet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mil_tweets_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mil_tweets_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type '_io.TextIOWrapper' has no len()"
     ]
    }
   ],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import unicodedata\n",
    "\n",
    "il_tweets_cleaned = open(\"may.txt\", \"w\")\n",
    "\n",
    "# Make text lowercase, remove text in square brackets, remove punctuation and remove \n",
    "# words containing numbers. \n",
    "with open(\"june.txt\", \"r\") as myFile:\n",
    "    for tweet in myFile:\n",
    "        parsed_tweet = js.loads(tweet)\n",
    "        text = parsed_tweet[\"text\"].lower()\n",
    "        text = re.sub('\\[.*?\\]', '', text)\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "        text = re.sub('\\w*\\d\\w*', '', text)\n",
    "        parsed_tweet[\"cleaned_text\"] = text\n",
    "        il_tweets_cleaned.write(js.dumps(parsed_tweet) + \"\\n\")\n",
    "il_tweets_cleaned.close()\n",
    "print(len(il_tweets_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Apply second round of text cleaning techniques which takes care of nonsensical text and additional punctuation\n",
    "il_tweets_cleaned2 = open(\"june_cleaned2.txt\", \"w\")\n",
    "\n",
    "with open(\"june_cleaned.txt\", \"r\") as myFile:\n",
    "    for tweet in myFile:\n",
    "        parsed_tweet = js.loads(tweet)\n",
    "        text = re.sub('[‘’“”…]', '', parsed_tweet[\"cleaned_text\"])\n",
    "        text = re.sub('\\n', ' ', text)\n",
    "        parsed_tweet[\"cleaned_text\"] = text\n",
    "        il_tweets_cleaned2.write(js.dumps(parsed_tweet) + \"\\n\")\n",
    "il_tweets_cleaned2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize each word in tweet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized = open(\"june_tokenized.txt\", \"w\")\n",
    "\n",
    "with open(\"june_cleaned2.txt\", \"r\") as myFile:\n",
    "    for tweet in myFile:\n",
    "        parsed_tweet = js.loads(tweet)\n",
    "        parsed_tweet[\"tokenized\"] = word_tokenize(parsed_tweet[\"cleaned_text\"])\n",
    "        tokenized.write(js.dumps(parsed_tweet) + \"\\n\")\n",
    "tokenized.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords in tweet\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "il_tweet_stopwords_removed = open(\"june_stopwords_removed.txt\", \"w\")\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    return [word for word in sentence if not word in stop_words]\n",
    "\n",
    "with open(\"june_tokenized.txt\", \"r\") as myFile:\n",
    "    for tweet in myFile:\n",
    "        parsed_tweet = js.loads(tweet)\n",
    "        parsed_tweet[\"stopwords_removed\"] = remove_stopwords(parsed_tweet[\"tokenized\"])\n",
    "        il_tweet_stopwords_removed.write(js.dumps(parsed_tweet) + \"\\n\")\n",
    "il_tweet_stopwords_removed.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the modified tokenized words\n",
    "\n",
    "joined = open(\"june_joined.txt\", \"w\")\n",
    "\n",
    "with open(\"june_stopwords_removed.txt\", \"r\") as myFile:\n",
    "    for tweet in myFile:\n",
    "        parsed_tweet = js.loads(tweet)\n",
    "        parsed_tweet[\"joined\"] = ' '.join(parsed_tweet[\"stopwords_removed\"])\n",
    "        joined.write(js.dumps(parsed_tweet) + \"\\n\")\n",
    "joined.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grab text and location from tweet\n",
    "\n",
    "# texts_and_locs = open(\"texts_and_location.txt\", \"w\")\n",
    "\n",
    "# with open(\"il_tweets_cleaned2.txt\", \"r\") as myFile:\n",
    "#     for tweet in myFile:\n",
    "#         parsed_tweet = js.loads(tweet)\n",
    "#         try:\n",
    "#             parsed_tweet[] = js.dumps({tweetjs[\"text\"] : tweetjs[\"place\"][\"full_name\"]})\n",
    "#             texts_and_locs.write(dic)\n",
    "#         except TypeError as e:\n",
    "#             continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleted the lemmatization as repetition of words in a single tweet is rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all text, their sentiment values and location in a text file.\n",
    "# apply SentimentIntensityAnalyzer on each tweet and set polarity scores \n",
    "# A SentimentAnalyzer is a tool to implement and facilitate Sentiment \n",
    "# Analysis tasks using NLTK features and classifiers, especially for teaching and demonstrative purposes.\n",
    "#VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis \n",
    "# tool that is specifically attuned to sentiments expressed in social media\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment_scores = open(\"june_sentiment_scores.txt\", \"w\")\n",
    "\n",
    "def analyse_sentiment():\n",
    "    with open(\"june_joined.txt\", \"r\") as myFile:\n",
    "        for tweet in myFile:\n",
    "            parsed_tweet = js.loads(tweet)\n",
    "            score = SentimentIntensityAnalyzer().polarity_scores(parsed_tweet[\"joined\"])\n",
    "            neg = score[\"neg\"]\n",
    "            pos = score[\"pos\"]\n",
    "            neu = score[\"neu\"]\n",
    "            if neu > neg and neu > pos:\n",
    "                parsed_tweet[\"sentiment\"] = \"Neutral\"\n",
    "            elif neg > pos:\n",
    "                parsed_tweet[\"sentiment\"] = \"Negative\"\n",
    "            elif pos > neg:\n",
    "                parsed_tweet[\"sentiment\"] = \"Positive\"\n",
    "            else:\n",
    "                parsed_tweet[\"sentiment\"] = \"Neutral\"\n",
    "            \n",
    "            parsed_tweet[\"sentiment_score\"] = score\n",
    "            sentiment_scores.write(js.dumps(parsed_tweet) + \"\\n\")\n",
    "analyse_sentiment()\n",
    "sentiment_scores.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city(city):\n",
    "    pattern = \"([^,]+)\"\n",
    "    match = re.match(pattern, city).group(0)\n",
    "    #If-statement after search() tests if it succeeded\n",
    "    return match\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "js_obj = []\n",
    "cities_and_states = open(\"june_cities_and_states.txt\", \"w\")\n",
    "\n",
    "with open(\"june_sentiment_scores.txt\", \"r\") as myFile:\n",
    "    for tweet in myFile:\n",
    "        js_obj.append(js.loads(tweet.strip()))\n",
    "\n",
    "for tweet in js_obj:\n",
    "    tweet[\"city\"] = get_city(tweet[\"place\"][\"full_name\"])\n",
    "    cities_and_states.write(js.dumps(tweet) + \"\\n\")\n",
    "cities_and_states.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe from US cities and name it us_cities\n",
    "# get county fips from us_cities and create fips tag in tweet\n",
    "df = pd.read_csv(\"us_cities.csv\")\n",
    "\n",
    "fips = open(\"june_fips.txt\", \"w\")\n",
    "unmatched_cities = []\n",
    "\n",
    "with open(\"june_cities_and_states.txt\", \"r\") as myFile:\n",
    "    for tweet in myFile:\n",
    "        t = js.loads(tweet)\n",
    "        city = t[\"city\"]\n",
    "        state = \"IL\"\n",
    "        matched_city = df[(df[\"city\"] == city) & (df[\"state_id\"] == state)]\n",
    "        if (len(matched_city) != 1):\n",
    "            unmatched_cities.append(tweet)\n",
    "            continue\n",
    "        t[\"county_fips\"] = str(matched_city[\"county_fips\"].iloc[0])\n",
    "        fips.write(js.dumps(t) + \"\\n\")\n",
    "fips.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "print(len(unmatched_cities))\n",
    "\n",
    "# with open(\"fips.txt\", \"r\") as myFile:\n",
    "#     for tweet in myFile:\n",
    "#         t = js.loads(tweet)\n",
    "#         if (t[\"county_fips\"] not in na_fips[\"county_fips\"]): \n",
    "#             if(na_fips['state'] == 'IL'):\n",
    "#                 t[\"county_fips\"] = na_fips[\"county_fips\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Westmont': 9, 'Northbrook': 36, 'Chicago': 3985, 'Grayslake': 6, 'Schaumburg': 43, 'Evanston': 117, 'Springfield': 110, 'Robinson': 3, 'Kankakee': 13, 'Oswego': 13, 'Sugar Grove': 4, 'Elk Grove Village': 20, 'Berwyn': 6, 'North Barrington': 5, 'Antioch': 11, 'DeKalb': 15, 'Wheaton': 69, 'New Lenox': 8, 'Geneva': 8, 'Bartlett': 19, 'Forsyth': 3, 'Glen Ellyn': 9, 'Normal': 46, 'Cicero': 6, 'Metropolis': 2, 'Frankfort': 48, 'Naperville': 110, 'Mundelein': 12, 'Itasca': 19, 'Champaign': 48, 'Joliet': 53, 'Darien': 6, 'Elgin': 68, 'Rockford': 62, 'Gilberts': 15, 'Manteno': 2, 'Addison': 8, 'Orland Park': 33, 'Lombard': 36, 'Lincolnshire': 4, 'Hinsdale': 12, 'Round Lake': 7, 'Skokie': 25, 'Crystal Lake': 15, 'Des Plaines': 32, 'Freeport': 12, 'Warrenville': 11, 'Alton': 10, 'Hoffman Estates': 47, 'Harvard': 14, 'Norridge': 4, 'Sycamore': 5, 'Waterloo': 7, 'Bedford Park': 2, 'Hanover Park': 12, 'Downers Grove': 43, 'Tinley Park': 18, 'Huntley': 40, 'Sauk Village': 1, 'Winfield': 11, 'Bartonville': 12, 'Barrington': 7, 'Gibson City': 1, 'Oak Park': 68, 'Schiller Park': 8, 'Brookfield': 11, 'Belleville': 31, 'Lindenhurst': 7, 'Park Forest': 34, \"O'Fallon\": 20, 'Bolingbrook': 28, 'Shorewood': 24, 'Arlington Heights': 50, 'Carbondale': 19, 'Batavia': 19, 'Lisle': 9, 'Glendale Heights': 4, 'Streamwood': 9, 'Palatine': 10, 'Forest Park': 7, 'South Elgin': 10, 'Lake Forest': 10, 'Urbana': 52, 'Woodridge': 19, 'Willow Springs': 2, 'Justice': 2, 'Lakewood': 6, 'Decatur': 32, 'East Moline': 9, 'Mahomet': 3, 'Machesney Park': 1, 'Crestwood': 8, 'Vernon Hills': 7, 'Georgetown': 1, 'Deerfield': 9, 'Oak Forest': 2, 'Franklin Park': 13, 'Mascoutah': 2, 'Oakbrook Terrace': 3, 'Melrose Park': 9, 'Maryville': 15, 'Winnebago': 1, 'Glen Carbon': 13, 'Mount Prospect': 9, 'Zion': 3, 'Collinsville': 5, 'Crest Hill': 5, 'Lockport': 5, 'Glenview': 19, 'Homewood': 11, 'Romeoville': 7, 'Charleston': 4, 'Genoa': 10, 'Moline': 23, 'Roxana': 1, 'Wilmette': 41, 'Lincolnwood': 5, 'Flossmoor': 9, 'Palos Park': 9, 'Roscoe': 3, 'Morris': 22, 'Roselle': 5, 'Burnham': 16, 'Niles': 10, 'McCook': 3, 'Swansea': 3, 'Palos Heights': 2, 'Waukegan': 26, 'Morton Grove': 2, 'Riverton': 1, 'Country Club Hills': 11, 'Lansing': 10, 'Chicago Ridge': 4, 'West Chicago': 13, 'Elmhurst': 27, 'Fox Lake': 7, 'Cherry Valley': 5, 'North Riverside': 2, 'Highland Park': 10, 'La Grange Park': 11, 'Streator': 1, 'Rock Island': 9, 'Harwood Heights': 3, 'Bannockburn': 2, 'Bloomingdale': 6, 'Kildeer': 4, 'Frankfort Square': 3, 'Winnetka': 5, 'Evergreen Park': 7, 'Sparta': 1, 'Villa Park': 8, 'Oak Lawn': 22, 'East Peoria': 7, 'Heyworth': 1, 'Countryside': 4, 'Mount Zion': 1, 'Port Byron': 4, 'Coal Valley': 3, 'River Forest': 13, 'Crete': 1, 'Indian Head Park': 8, 'Glencoe': 6, 'Galena': 3, 'Minooka': 4, 'Cary': 7, 'Round Lake Park': 8, 'Pingree Grove': 4, 'Monmouth': 4, 'Galesburg': 9, 'Bellwood': 2, 'Peotone': 1, 'Park Ridge': 15, 'Gurnee': 12, 'Edwardsville': 14, 'Wadsworth': 1, 'Lake Zurich': 13, 'Wood Dale': 3, 'Buffalo Grove': 15, 'Channahon': 1, 'New Baden': 2, 'Midlothian': 3, 'Granite City': 8, 'Lincoln': 3, 'Ottawa': 4, 'Carlyle': 4, 'Carol Stream': 8, 'South Holland': 10, 'LaSalle': 7, 'Kewanee': 4, 'Eldorado': 6, 'Bridgeview': 4, 'Park City': 15, 'Orland Hills': 5, 'Spring Grove': 7, 'Plainfield': 20, 'Du Quoin': 3, 'Summit': 4, 'Le Roy': 4, 'Lakemoor': 1, 'Mokena': 12, 'Calumet City': 5, 'Burbank': 8, 'Silvis': 3, 'Fairview Heights': 6, 'Troy': 10, 'Quincy': 11, 'Dolton': 5, 'Elmwood Park': 5, 'Aledo': 1, 'Johnsburg': 2, 'Willowbrook': 6, 'Lynwood': 6, 'Loves Park': 1, 'Rockdale': 1, 'Northfield': 3, 'East Dundee': 1, 'Danville': 2, 'Riverwoods': 2, 'Round Lake Beach': 4, 'Hillside': 4, 'Yorkville': 5, 'Bradley': 5, 'Campton Hills': 3, 'Germantown Hills': 3, 'Savoy': 4, 'Broadview': 2, 'Bourbonnais': 8, 'Towanda': 1, 'Glenwood': 3, 'Belvidere': 5, 'Sterling': 7, 'Herrin': 8, 'Matteson': 3, 'Cahokia': 4, 'East Alton': 2, 'Lake in the Hills': 11, 'Hodgkins': 7, 'Morton': 4, 'Wheeling': 9, 'Chicago Heights': 2, 'Columbia': 2, 'Wauconda': 7, 'Gages Lake': 3, 'Morrison': 3, 'Harvey': 5, 'Pekin': 2, 'Newton': 1, 'Riverdale': 1, 'East Hazel Crest': 1, 'Marseilles': 1, 'South Beloit': 1, 'Westchester': 6, 'Burr Ridge': 8, 'Richton Park': 6, 'Pana': 1, 'Grandview': 1, 'Benton': 1, 'Olney': 1, 'Long Grove': 7, 'Peru': 8, 'Paris': 2, 'Maywood': 3, 'Western Springs': 4, 'Hartford': 1, 'North Chicago': 7, 'Clinton': 3, 'Third Lake': 1, 'Highland': 1, 'Homer Glen': 2, 'Chatham': 3, 'Lake Villa': 5, 'Hampshire': 2, 'Southern View': 2, 'Shiloh': 2, 'Elwood': 1, 'Dixon': 1, 'Mattoon': 2, 'Caseyville': 1, 'Lawrenceville': 2, 'Lemont': 9, 'Pontiac': 1, 'Clarendon Hills': 4, 'Algonquin': 7, 'Washington': 2, 'Hainesville': 1, 'North Aurora': 2, 'Monticello': 2, 'South Barrington': 1, 'West Dundee': 4, 'Lyons': 1, 'Vandalia': 2, 'Alsip': 2, 'Oak Brook': 2, 'Pittsfield': 1, 'Davis Junction': 1, 'Libertyville': 2, 'Stone Park': 2, 'Tolono': 1, 'Chillicothe': 4, 'Riverside': 2, 'Geneseo': 1, 'Wood River': 3, 'McHenry': 4, 'Blue Island': 4, 'Lebanon': 3, 'Hickory Hills': 1, 'Sandwich': 2, 'Mount Vernon': 3, 'Carpentersville': 4, 'Greenville': 3, 'Casey': 5, 'Marion': 1, 'River Grove': 3, 'Macomb': 2, 'Carterville': 1, 'Wonder Lake': 2, 'Palos Hills': 2, 'East Dubuque': 1, 'Rochelle': 1, 'Hawthorn Woods': 2, 'Worth': 1, 'Galva': 1, 'Rolling Meadows': 2, 'Fairmont City': 1, 'Rosemont': 2, 'Wilmington': 1, 'Marquette Heights': 1, 'Manhattan': 1, 'Braidwood': 1, 'Bridgeport': 1, 'Bondville': 3, 'Sullivan': 1, 'Effingham': 1, 'Woodstock': 2, 'Anna': 1, 'Peoria Heights': 1, 'Mackinaw': 1, 'Lake Bluff': 1, 'Venice': 1, 'Madison': 1, 'Boulder Hill': 1, 'Twin Grove': 1, 'Tower Lakes': 1, 'Eureka': 1, 'Calumet Park': 1, 'Farmington': 1, 'Rapids City': 1, 'Wayne': 1, 'Colona': 1, 'Momence': 1, 'Henry': 1, 'Rock Falls': 1, 'Monee': 1, 'Petersburg': 1}\n",
      "354\n",
      "7186\n"
     ]
    }
   ],
   "source": [
    "# aggregate median score for each city\n",
    "\n",
    "repeated_cities = {}\n",
    "\n",
    "with open(\"june_fips.txt\", \"r\") as myFile:\n",
    "    i = 0\n",
    "    for tweet in myFile:\n",
    "        i += 1\n",
    "        t = js.loads(tweet)\n",
    "        if t[\"city\"] in repeated_cities:\n",
    "            repeated_cities[t[\"city\"]] += 1\n",
    "        else:\n",
    "            repeated_cities[t[\"city\"]] = 1\n",
    "    \n",
    "print(repeated_cities)\n",
    "print(len(repeated_cities))\n",
    "print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'june_fips.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-60d156a38df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentiment_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"june_fips.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmyFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'june_fips.txt'"
     ]
    }
   ],
   "source": [
    "sentiment_total = {}\n",
    "\n",
    "with open(\"june_fips.txt\", \"r\") as myFile:\n",
    "    for tweet in myFile:\n",
    "        t = js.loads(tweet)\n",
    "        if t[\"county_fips\"] in sentiment_total:\n",
    "            sentiment_total[t[\"county_fips\"]][\"neg\"] += t[\"sentiment_score\"][\"neg\"]\n",
    "            sentiment_total[t[\"county_fips\"]][\"neu\"] += t[\"sentiment_score\"][\"neu\"]\n",
    "            sentiment_total[t[\"county_fips\"]][\"pos\"] += t[\"sentiment_score\"][\"pos\"]\n",
    "            sentiment_total[t[\"county_fips\"]][\"total_tweets\"] += 1\n",
    "        else:\n",
    "            sentiment_total[t[\"county_fips\"]] = t[\"sentiment_score\"]\n",
    "            sentiment_total[t[\"county_fips\"]][\"total_tweets\"] = 1\n",
    "\n",
    "\n",
    "print(sentiment_total)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_fips = [17001,17003,17005,17007,17009,17011,17013,17015,17017,17019,17021,17023,17025,17027,17029, \n",
    "           17031,17033,17035,17039,17037,17041,17043,17045,17047,17049,17051,17053,17055,17057,17059,\n",
    "           17061,17063,17065,17067,17069,17071,17073,17075,17077,17079,17081,17083,17085,17087,17089,17091,17093,\n",
    "           17095,17097,17099,17101,17103,17105,17107,17115,17117,17119,17121,17123,17125,17127,17109,17111,17113,\n",
    "           17129,17131,17133,17135,17137,17139,17141,17143,17145,17147,17149,17151,17153,17155,17157,17159,17161,\n",
    "           17165,17167,17169,17171,17173,17163,17175,17177,17179,17181,17183,17185,17187,17189,17191,17193,17195,\n",
    "           17197,17199,17201,17203]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ba370c198d06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcounty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentiment_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"neg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total_county_score\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"average_per_county\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total_county_score\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total_tweets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentiment_total' is not defined"
     ]
    }
   ],
   "source": [
    "for county, sentiment in sentiment_total.items(): \n",
    "    neg = sentiment[\"neg\"]\n",
    "    pos = sentiment[\"pos\"]\n",
    "    sentiment[\"total_county_score\"] = (0 - neg) + pos\n",
    "    sentiment[\"average_per_county\"] = sentiment[\"total_county_score\"] / sentiment[\"total_tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'17019': {'neg': 1.4060000000000001, 'neu': 5.342, 'pos': 1.2530000000000001, 'compound': 0.6597, 'total_tweets': 8, 'total_county_score': -0.15300000000000002, 'average_per_county': -0.019125000000000003}, '17031': {'neg': 48.49699999999999, 'neu': 228.91199999999995, 'pos': 49.593999999999994, 'compound': 0.1531, 'total_tweets': 331, 'total_county_score': 1.0970000000000013, 'average_per_county': 0.003314199395770397}, '17161': {'neg': 1.106, 'neu': 4.9430000000000005, 'pos': 1.9520000000000002, 'compound': 0.4404, 'total_tweets': 8, 'total_county_score': 0.8460000000000001, 'average_per_county': 0.10575000000000001}, '17167': {'neg': 0.46599999999999997, 'neu': 3.979, 'pos': 0.5549999999999999, 'compound': 0.0, 'total_tweets': 5, 'total_county_score': 0.08899999999999997, 'average_per_county': 0.017799999999999993}, '17115': {'neg': 0.508, 'neu': 0.275, 'pos': 0.216, 'compound': -0.8131, 'total_tweets': 1, 'total_county_score': -0.29200000000000004, 'average_per_county': -0.29200000000000004}, '17097': {'neg': 2.9530000000000007, 'neu': 10.600999999999999, 'pos': 2.4459999999999997, 'compound': 0.0, 'total_tweets': 16, 'total_county_score': -0.507000000000001, 'average_per_county': -0.03168750000000006}, '17201': {'neg': 0.133, 'neu': 2.867, 'pos': 0.0, 'compound': 0.0, 'total_tweets': 3, 'total_county_score': -0.133, 'average_per_county': -0.044333333333333336}, '17197': {'neg': 3.305, 'neu': 9.188999999999998, 'pos': 2.506, 'compound': 0.6369, 'total_tweets': 16, 'total_county_score': -0.7990000000000004, 'average_per_county': -0.049937500000000024}, '17093': {'neg': 0.322, 'neu': 0.222, 'pos': 0.456, 'compound': 0.296, 'total_tweets': 1, 'total_county_score': 0.134, 'average_per_county': 0.134}, '17119': {'neg': 0.456, 'neu': 3.364, 'pos': 0.179, 'compound': 0.0, 'total_tweets': 4, 'total_county_score': -0.277, 'average_per_county': -0.06925}, '17043': {'neg': 2.086, 'neu': 22.297, 'pos': 4.617, 'compound': 0.3182, 'total_tweets': 29, 'total_county_score': 2.531, 'average_per_county': 0.08727586206896552}, '17091': {'neg': 0.34, 'neu': 0.66, 'pos': 0.0, 'compound': -0.5574, 'total_tweets': 1, 'total_county_score': -0.34, 'average_per_county': -0.34}, '17113': {'neg': 0.44199999999999995, 'neu': 4.668, 'pos': 0.89, 'compound': 0.4019, 'total_tweets': 6, 'total_county_score': 0.44800000000000006, 'average_per_county': 0.07466666666666667}, '17199': {'neg': 0.0, 'neu': 1.615, 'pos': 0.385, 'compound': 0.6124, 'total_tweets': 2, 'total_county_score': 0.385, 'average_per_county': 0.1925}, '17089': {'neg': 1.326, 'neu': 4.744, 'pos': 0.9299999999999999, 'compound': -0.4939, 'total_tweets': 7, 'total_county_score': -0.39600000000000013, 'average_per_county': -0.05657142857142859}, '17165': {'neg': 0.259, 'neu': 0.741, 'pos': 0.0, 'compound': -0.6369, 'total_tweets': 1, 'total_county_score': -0.259, 'average_per_county': -0.259}, '17111': {'neg': 1.082, 'neu': 5.335999999999999, 'pos': 1.583, 'compound': 0.2732, 'total_tweets': 8, 'total_county_score': 0.5009999999999999, 'average_per_county': 0.06262499999999999}, '17023': {'neg': 0.0, 'neu': 0.625, 'pos': 0.375, 'compound': 0.2023, 'total_tweets': 1, 'total_county_score': 0.375, 'average_per_county': 0.375}, '17063': {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0, 'total_tweets': 1, 'total_county_score': 0.0, 'average_per_county': 0.0}, '17163': {'neg': 0.0, 'neu': 2.6799999999999997, 'pos': 0.32, 'compound': 0.0, 'total_tweets': 3, 'total_county_score': 0.32, 'average_per_county': 0.10666666666666667}, '17037': {'neg': 0.151, 'neu': 2.635, 'pos': 0.214, 'compound': 0.0, 'total_tweets': 3, 'total_county_score': 0.063, 'average_per_county': 0.021}, '17101': {'neg': 0.185, 'neu': 0.815, 'pos': 0.0, 'compound': -0.3612, 'total_tweets': 1, 'total_county_score': -0.185, 'average_per_county': -0.185}, '17085': {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0, 'total_tweets': 1, 'total_county_score': 0.0, 'average_per_county': 0.0}, '17001': {'neg': 0.0, 'neu': 0.803, 'pos': 0.197, 'compound': 0.4019, 'total_tweets': 1, 'total_county_score': 0.197, 'average_per_county': 0.197}, '17051': {'neg': 0.298, 'neu': 0.526, 'pos': 0.175, 'compound': -0.34, 'total_tweets': 1, 'total_county_score': -0.123, 'average_per_county': -0.123}, '17179': {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.765, 'total_tweets': 1, 'total_county_score': 0.487, 'average_per_county': 0.487}, '17135': {'neg': 0.0, 'neu': 0.433, 'pos': 0.567, 'compound': 0.8957, 'total_tweets': 1, 'total_county_score': 0.567, 'average_per_county': 0.567}, '17109': {'neg': 0.0, 'neu': 0.769, 'pos': 0.231, 'compound': 0.34, 'total_tweets': 1, 'total_county_score': 0.231, 'average_per_county': 0.231}, '17145': {'neg': 0.348, 'neu': 0.652, 'pos': 0.0, 'compound': -0.1531, 'total_tweets': 1, 'total_county_score': -0.348, 'average_per_county': -0.348}, '17077': {'neg': 0.0, 'neu': 0.323, 'pos': 0.677, 'compound': 0.2732, 'total_tweets': 1, 'total_county_score': 0.677, 'average_per_county': 0.677}, 17001: {'total_county_score': 'NA'}, 17003: {'total_county_score': 'NA'}, 17005: {'total_county_score': 'NA'}, 17007: {'total_county_score': 'NA'}, 17009: {'total_county_score': 'NA'}, 17011: {'total_county_score': 'NA'}, 17013: {'total_county_score': 'NA'}, 17015: {'total_county_score': 'NA'}, 17017: {'total_county_score': 'NA'}, 17019: {'total_county_score': 'NA'}, 17021: {'total_county_score': 'NA'}, 17023: {'total_county_score': 'NA'}, 17025: {'total_county_score': 'NA'}, 17027: {'total_county_score': 'NA'}, 17029: {'total_county_score': 'NA'}, 17031: {'total_county_score': 'NA'}, 17033: {'total_county_score': 'NA'}, 17035: {'total_county_score': 'NA'}, 17039: {'total_county_score': 'NA'}, 17037: {'total_county_score': 'NA'}, 17041: {'total_county_score': 'NA'}, 17043: {'total_county_score': 'NA'}, 17045: {'total_county_score': 'NA'}, 17047: {'total_county_score': 'NA'}, 17049: {'total_county_score': 'NA'}, 17051: {'total_county_score': 'NA'}, 17053: {'total_county_score': 'NA'}, 17055: {'total_county_score': 'NA'}, 17057: {'total_county_score': 'NA'}, 17059: {'total_county_score': 'NA'}, 17061: {'total_county_score': 'NA'}, 17063: {'total_county_score': 'NA'}, 17065: {'total_county_score': 'NA'}, 17067: {'total_county_score': 'NA'}, 17069: {'total_county_score': 'NA'}, 17071: {'total_county_score': 'NA'}, 17073: {'total_county_score': 'NA'}, 17075: {'total_county_score': 'NA'}, 17077: {'total_county_score': 'NA'}, 17079: {'total_county_score': 'NA'}, 17081: {'total_county_score': 'NA'}, 17083: {'total_county_score': 'NA'}, 17085: {'total_county_score': 'NA'}, 17087: {'total_county_score': 'NA'}, 17089: {'total_county_score': 'NA'}, 17091: {'total_county_score': 'NA'}, 17093: {'total_county_score': 'NA'}, 17095: {'total_county_score': 'NA'}, 17097: {'total_county_score': 'NA'}, 17099: {'total_county_score': 'NA'}, 17101: {'total_county_score': 'NA'}, 17103: {'total_county_score': 'NA'}, 17105: {'total_county_score': 'NA'}, 17107: {'total_county_score': 'NA'}, 17115: {'total_county_score': 'NA'}, 17117: {'total_county_score': 'NA'}, 17119: {'total_county_score': 'NA'}, 17121: {'total_county_score': 'NA'}, 17123: {'total_county_score': 'NA'}, 17125: {'total_county_score': 'NA'}, 17127: {'total_county_score': 'NA'}, 17109: {'total_county_score': 'NA'}, 17111: {'total_county_score': 'NA'}, 17113: {'total_county_score': 'NA'}, 17129: {'total_county_score': 'NA'}, 17131: {'total_county_score': 'NA'}, 17133: {'total_county_score': 'NA'}, 17135: {'total_county_score': 'NA'}, 17137: {'total_county_score': 'NA'}, 17139: {'total_county_score': 'NA'}, 17141: {'total_county_score': 'NA'}, 17143: {'total_county_score': 'NA'}, 17145: {'total_county_score': 'NA'}, 17147: {'total_county_score': 'NA'}, 17149: {'total_county_score': 'NA'}, 17151: {'total_county_score': 'NA'}, 17153: {'total_county_score': 'NA'}, 17155: {'total_county_score': 'NA'}, 17157: {'total_county_score': 'NA'}, 17159: {'total_county_score': 'NA'}, 17161: {'total_county_score': 'NA'}, 17165: {'total_county_score': 'NA'}, 17167: {'total_county_score': 'NA'}, 17169: {'total_county_score': 'NA'}, 17171: {'total_county_score': 'NA'}, 17173: {'total_county_score': 'NA'}, 17163: {'total_county_score': 'NA'}, 17175: {'total_county_score': 'NA'}, 17177: {'total_county_score': 'NA'}, 17179: {'total_county_score': 'NA'}, 17181: {'total_county_score': 'NA'}, 17183: {'total_county_score': 'NA'}, 17185: {'total_county_score': 'NA'}, 17187: {'total_county_score': 'NA'}, 17189: {'total_county_score': 'NA'}, 17191: {'total_county_score': 'NA'}, 17193: {'total_county_score': 'NA'}, 17195: {'total_county_score': 'NA'}, 17197: {'total_county_score': 'NA'}, 17199: {'total_county_score': 'NA'}, 17201: {'total_county_score': 'NA'}, 17203: {'total_county_score': 'NA'}}\n"
     ]
    }
   ],
   "source": [
    "# for fips in il_fips: \n",
    "#     if fips not in sentiment_total:\n",
    "#         sentiment_total[fips] = {}\n",
    "#         sentiment_total[fips] = \"NA\"\n",
    "        \n",
    "# print(sentiment_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-151c673f5bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfips_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfips\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentiment_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfips_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"FIPS\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfips\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sentiment Average\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"average_per_county\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentiment_total' is not defined"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fips_sentiment = []\n",
    "print(sentiment_total)\n",
    "for fips, sentiment in sentiment_total.items(): \n",
    "    fips_sentiment.append({\"FIPS\" : fips, \"Sentiment Average\" : sentiment[\"average_per_county\"]})\n",
    "\n",
    "print(len(fips_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fips_sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0b9d64417410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfips_sentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fips_sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "dframe = pd.DataFrame(fips_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FIPS  Sentiment Average\n",
      "0  17043           0.011387\n",
      "1  17031           0.019545\n",
      "2  17097           0.018622\n",
      "3  17167           0.029923\n",
      "4  17033           0.000000\n"
     ]
    }
   ],
   "source": [
    "print(dframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-97d122f67ca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mendpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FIPS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment Average'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dframe' is not defined"
     ]
    }
   ],
   "source": [
    "colorscale = [\n",
    "    \"#e813ae\", \n",
    "    \"#d213bb\", \n",
    "    \"#ba13c7\", \n",
    "    \"#9f13d3\", \n",
    "    \"#7d13de\", \n",
    "    \"#4e13e8\", \n",
    "    \"#4c69d0\", \n",
    "    \"#4a93b4\", \n",
    "    \"#48b493\", \n",
    "    \"#45d069\", \n",
    "    \"#43e813\",\n",
    "    \"#000000\"\n",
    "]\n",
    "\n",
    "endpts = list(np.linspace(-1, 1, num=11))\n",
    "fips = dframe['FIPS'].tolist()\n",
    "values = dframe['Sentiment Average'].tolist()\n",
    "\n",
    "fig = ff.create_choropleth(\n",
    "    fips=fips, values=values, scope=['IL'],\n",
    "    binning_endpoints=endpts, colorscale=colorscale,\n",
    "    show_state_data=False,\n",
    "    show_hover=True,\n",
    "    asp = 2.9,\n",
    "    title_text = 'Average Sentiment of Tweets per County in June',\n",
    "    legend_title = 'Sentiment Average'\n",
    ")\n",
    "\n",
    "fig.update_geos(showcountries=False, showcoastlines=False, showland=True, fitbounds='locations')\n",
    "\n",
    "fig.layout.template = None\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
